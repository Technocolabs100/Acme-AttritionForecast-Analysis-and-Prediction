#AcmeForecast Data:-

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

file_path = '/WA_Fn-UseC_-HR-Employee-Attrition.csv'
df = pd.read_csv(file_path)
df.head()
numerical_summary = df.describe()

categorical_summary = df.describe(include=['object'])

missing_values = df.isnull().sum()

numerical_summary, categorical_summary, missing_values

df['Attrition'].value_counts()

df['EducationField'].unique()

df['Gender'].value_counts()

df.columns

df.duplicated()
# Compute the correlation matrix
numerical_df = df.select_dtypes(include=['number'])
corr_matrix = numerical_df.corr()

plt.figure(figsize=(14, 10))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()


numerical_df.skew()
label_encoder = LabelEncoder()
df['Attrition'] = label_encoder.fit_transform(df['Attrition'])
df = df.drop(columns=['EmployeeNumber', 'Over18', 'EmployeeCount', 'StandardHours'])

categorical_cols = df.select_dtypes(include=['object']).columns
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

X = df.drop('Attrition', axis=1)
y = df['Attrition']

#Linear-regression analysis:

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    return accuracy, precision, recall, f1

lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)
lr_metrics = evaluate_model(lr, X_test, y_test)
#Random-Forest model

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
rf_metrics = evaluate_model(rf, X_test, y_test)
#GBC

gb = GradientBoostingClassifier(random_state=42)
gb.fit(X_train, y_train)
gb_metrics = evaluate_model(gb, X_test, y_test)
print("Model Evaluation Metrics:")
print("Logistic Regression: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}".format(*lr_metrics))
print("Random Forest: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}".format(*rf_metrics))
print("Gradient Boosting: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}".format(*gb_metrics))

print("\nClassification Reports:")
print("Logistic Regression:")
print(classification_report(y_test, lr.predict(X_test)))

print("Random Forest:")
print(classification_report(y_test, rf.predict(X_test)))

print("Gradient Boosting:")
print(classification_report(y_test, gb.predict(X_test)))

# Insights: Overall Accuracy:-

# Logistic Regression: 86.05%
# Random Forest: 83.33%
# Gradient Boosting: 85.03%
# Logistic Regression has the highest overall accuracy, indicating it correctly classifies the majority of the instances.
